{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "360a0243-86c5-4cc0-b3e4-3a4b7da5310f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 13 03:11:04 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 537.70                 Driver Version: 537.70       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   39C    P8               4W /  35W |      0MiB /  6144MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     73796      C   ...nvs\\testing_finetune_llm\\python.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6539cfae-d55a-4c1c-b218-808b5c469019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9d7e8cf-beee-41c6-a23d-1e2573249a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\.conda\\envs\\testing_finetune_llm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "# import config\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "# import jsonlines\n",
    "\n",
    "from utilities import *\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TrainingArguments\n",
    "# from transformers import AutoModelForCausalLM\n",
    "# from llama import BasicModelRunner\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "global_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1111962f-a4de-4612-af7b-8e07ed69be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "\n",
    "filename = \"dataset/lamini_docs.jsonl\"\n",
    "model_name = \"EleutherAI/pythia-70m\"\n",
    "model_save_path = f\"saved_base_model_with_model_name_{model_name}.pth\"\n",
    "max_steps = 3\n",
    "trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n",
    "output_dir = trained_model_name\n",
    "\n",
    "# dataset_name = \"lamini_docs.jsonl\"\n",
    "# dataset_path = f\"/content/{dataset_name}\"\n",
    "# use_hf = False\n",
    "dataset_path = \"lamini/lamini_docs\"\n",
    "use_hf = False #True\n",
    "\n",
    "training_config = {\n",
    "    \"model\": {\n",
    "        \"pretrained_name\": model_name,\n",
    "        \"max_length\" : 2048\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"use_hf\": use_hf,\n",
    "        \"path\": filename #dataset_path\n",
    "    },\n",
    "    \"verbose\": True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46464c56-c63b-4428-bebc-1abf9114fedb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Download dataset from hugging face hub\n",
    "\n",
    "dataset_path = \"lamini/lamini_docs\"\n",
    "dataset = datasets.load_dataset(dataset_path)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbd26471-f574-4098-92f1-bc8e1d4443cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1260\n",
      "})\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 140\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# load the dataset from local and split the dataset into training and testing set\n",
    "\n",
    "finetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=filename, split=\"train\")\n",
    "split_dataset = finetuning_dataset_loaded.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
    "\n",
    "train_dataset, test_dataset = split_dataset['train'], split_dataset['test']\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e7468f8-2e88-460e-b8e2-92744a63ef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\.conda\\envs\\testing_finetune_llm\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e75a3354-e41d-43f4-9329-3a01115fbc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select GPU device\n"
     ]
    }
   ],
   "source": [
    "device_count = torch.cuda.device_count()\n",
    "if device_count > 0:\n",
    "    # logger.debug(\"Select GPU device\")\n",
    "    print(\"Select GPU device\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    # logger.debug(\"Select CPU device\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e96e303-7ac6-45db-9a2f-4e5180fb198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from trasnformer (hugging face hub)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "300b1909-2c85-4a34-a517-1d23ea07c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pretrained model and tokenizer from local\n",
    "\n",
    "save_directory = f\"pretrain_model/{model_name}\"\n",
    "model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac145876-3615-4930-9f12-2144c883d092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dd32b7b-ecc8-4805-ba29-affad7e95a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "  # Tokenize\n",
    "  input_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors=\"pt\",\n",
    "          truncation=True,\n",
    "          max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "    input_ids=input_ids.to(device),\n",
    "    max_length=max_output_tokens\n",
    "  )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "  # Strip the prompt\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "  return generated_text_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6f0c2a3-f896-4b06-a08e-d2b0e7d68fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test): Is it possible to fine-tune Lamini on a specific dataset for text generation in legal documents?\n",
      "Correct answer from Lamini docs: Lamini’s LLM Engine can help you fine-tune any model on huggingface or any OpenAI model.\n",
      "Model's answer: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A:\n",
      "\n",
      "I think you should use the following code:\n",
      "public class TextFile : File\n",
      "{\n",
      "    public string FileName { get; set; }\n",
      "    public string TextFileName { get; set; }\n",
      "    public string TextFileName { get; set; }\n",
      "    public string TextFileName { get; set; }\n",
      "    public\n"
     ]
    }
   ],
   "source": [
    "# if download dataset from hugging face \n",
    "\n",
    "# test_text = dataset['test'][0]['question']\n",
    "# print(\"Question input (test):\", test_text)\n",
    "# print(f\"Correct answer from Lamini docs: {dataset['test'][0]['answer']}\")\n",
    "# print(\"Model's answer: \")\n",
    "# print(inference(test_text, base_model1, tokenizer))\n",
    "\n",
    "# if data is loaded from local\n",
    "test_text = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_text)\n",
    "print(f\"Correct answer from Lamini docs: {test_dataset[0]['answer']}\")\n",
    "print(\"Model's answer: \")\n",
    "print(inference(test_text, base_model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bb9469c-1ac0-432a-a8ec-1638f5ec13d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "  # Learning rate\n",
    "  learning_rate=1.0e-5,\n",
    "\n",
    "  # Number of training epochs\n",
    "  num_train_epochs=10,\n",
    "\n",
    "  # Max steps to train for (each step is a batch of data)\n",
    "  # Overrides num_train_epochs, if not -1\n",
    "  # max_steps=1,\n",
    "\n",
    "  # Batch size for training\n",
    "  per_device_train_batch_size=1,\n",
    "\n",
    "  # Directory to save model checkpoints\n",
    "  output_dir=output_dir,\n",
    "\n",
    "  # Other arguments\n",
    "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "  disable_tqdm=False, # Disable progress bars\n",
    "  eval_steps=120, # Number of update steps between two evaluations\n",
    "  save_steps=120, # After # steps model is saved\n",
    "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "  evaluation_strategy=\"steps\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=1,\n",
    "  optim=\"adafactor\",\n",
    "  gradient_accumulation_steps = 4,\n",
    "  gradient_checkpointing=False,\n",
    "\n",
    "  # Parameters for early stopping\n",
    "  load_best_model_at_end=True,\n",
    "  save_total_limit=1,\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d5afd91-b19a-4074-a221-c1fd9e1caff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 512)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      ")\n",
      "Memory footprint 0.30687256 GB\n",
      "Flops 2195.667812352 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "model_flops = (\n",
    "  base_model.floating_point_ops(\n",
    "    {\n",
    "       \"input_ids\": torch.zeros(\n",
    "           (1, training_config[\"model\"][\"max_length\"])\n",
    "      )\n",
    "    }\n",
    "  )\n",
    "  * training_args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(base_model)\n",
    "print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
    "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21474c03-388f-43e3-b145-a4301f419e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\.conda\\envs\\testing_finetune_llm\\lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    model_flops=model_flops,\n",
    "    total_steps=max_steps,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfe0906-0818-4abf-b358-a651d1415fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ea1bf1a-e8d3-4f87-b58f-a275ac883f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  {'input_ids': tensor([], device='cuda:0', size=(1, 0)), 'attention_mask': tensor([], device='cuda:0', size=(1, 0)), 'labels': tensor([], device='cuda:0', size=(1, 0))}\n",
      "Inputs - input_ids tensor([], device='cuda:0', size=(1, 0))\n",
      "numel 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3150' max='3150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3150/3150 11:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.600107</td>\n",
       "      <td>2.462791</td>\n",
       "      <td>-25.520080</td>\n",
       "      <td>10066313976440.591797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.389148</td>\n",
       "      <td>2.282554</td>\n",
       "      <td>-52.177435</td>\n",
       "      <td>9973147767949.662109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.138661</td>\n",
       "      <td>2.218792</td>\n",
       "      <td>-78.570429</td>\n",
       "      <td>9976442985052.505859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.969670</td>\n",
       "      <td>2.166470</td>\n",
       "      <td>-104.926129</td>\n",
       "      <td>9981627576097.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.886782</td>\n",
       "      <td>2.103992</td>\n",
       "      <td>-131.080618</td>\n",
       "      <td>10000057256322.964844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.710067</td>\n",
       "      <td>2.111018</td>\n",
       "      <td>-157.078937</td>\n",
       "      <td>10022310150428.849609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.651831</td>\n",
       "      <td>2.084747</td>\n",
       "      <td>-182.859735</td>\n",
       "      <td>10050183863467.626953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.593966</td>\n",
       "      <td>2.087108</td>\n",
       "      <td>-208.832759</td>\n",
       "      <td>10061898847676.244141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1.476995</td>\n",
       "      <td>2.077676</td>\n",
       "      <td>-234.760329</td>\n",
       "      <td>10072972061861.523438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.405734</td>\n",
       "      <td>2.071707</td>\n",
       "      <td>-260.744949</td>\n",
       "      <td>10079636737016.169922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>1.329600</td>\n",
       "      <td>2.125621</td>\n",
       "      <td>-286.992949</td>\n",
       "      <td>10075838177815.376953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>1.287928</td>\n",
       "      <td>2.101751</td>\n",
       "      <td>-312.805796</td>\n",
       "      <td>10086688570923.441406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>1.284631</td>\n",
       "      <td>2.139304</td>\n",
       "      <td>-338.942065</td>\n",
       "      <td>10086251118018.423828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>1.171977</td>\n",
       "      <td>2.169505</td>\n",
       "      <td>-364.833290</td>\n",
       "      <td>10092650607657.894531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.130027</td>\n",
       "      <td>2.146213</td>\n",
       "      <td>-390.805960</td>\n",
       "      <td>10096097460199.498047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>1.101516</td>\n",
       "      <td>2.193984</td>\n",
       "      <td>-416.805746</td>\n",
       "      <td>10098457706346.607422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>1.043396</td>\n",
       "      <td>2.216930</td>\n",
       "      <td>-442.857780</td>\n",
       "      <td>10099349121610.101562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>1.023955</td>\n",
       "      <td>2.210157</td>\n",
       "      <td>-468.873272</td>\n",
       "      <td>10100928670697.707031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.969312</td>\n",
       "      <td>2.305628</td>\n",
       "      <td>-495.020989</td>\n",
       "      <td>10099643686693.460938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.941776</td>\n",
       "      <td>2.259723</td>\n",
       "      <td>-521.212340</td>\n",
       "      <td>10097642245348.322266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.937448</td>\n",
       "      <td>2.281542</td>\n",
       "      <td>-547.140022</td>\n",
       "      <td>10100697549854.583984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.891268</td>\n",
       "      <td>2.322105</td>\n",
       "      <td>-573.359152</td>\n",
       "      <td>10098340626421.533203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.858343</td>\n",
       "      <td>2.339171</td>\n",
       "      <td>-599.392757</td>\n",
       "      <td>10099314829754.707031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.830221</td>\n",
       "      <td>2.363046</td>\n",
       "      <td>-625.538828</td>\n",
       "      <td>10098391992868.925781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.781595</td>\n",
       "      <td>2.389295</td>\n",
       "      <td>-651.617662</td>\n",
       "      <td>10098585125668.726562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.830319</td>\n",
       "      <td>2.389486</td>\n",
       "      <td>-677.701442</td>\n",
       "      <td>10098689691657.921875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  {'input_ids': tensor([], device='cuda:0', size=(1, 0)), 'attention_mask': tensor([], device='cuda:0', size=(1, 0)), 'labels': tensor([], device='cuda:0', size=(1, 0))}\n",
      "Inputs - input_ids tensor([], device='cuda:0', size=(1, 0))\n",
      "numel 0\n",
      "Inputs:  {'input_ids': tensor([], device='cuda:0', size=(1, 0)), 'attention_mask': tensor([], device='cuda:0', size=(1, 0)), 'labels': tensor([], device='cuda:0', size=(1, 0))}\n",
      "Inputs - input_ids tensor([], device='cuda:0', size=(1, 0))\n",
      "numel 0\n",
      "Inputs:  {'input_ids': tensor([], device='cuda:0', size=(1, 0)), 'attention_mask': tensor([], device='cuda:0', size=(1, 0)), 'labels': tensor([], device='cuda:0', size=(1, 0))}\n",
      "Inputs - input_ids tensor([], device='cuda:0', size=(1, 0))\n",
      "numel 0\n",
      "Inputs:  {'input_ids': tensor([], device='cuda:0', size=(1, 0)), 'attention_mask': tensor([], device='cuda:0', size=(1, 0)), 'labels': tensor([], device='cuda:0', size=(1, 0))}\n",
      "Inputs - input_ids tensor([], device='cuda:0', size=(1, 0))\n",
      "numel 0\n",
      "Inputs:  {'input_ids': tensor([], device='cuda:0', size=(1, 0)), 'attention_mask': tensor([], device='cuda:0', size=(1, 0)), 'labels': tensor([], device='cuda:0', size=(1, 0))}\n",
      "Inputs - input_ids tensor([], device='cuda:0', size=(1, 0))\n",
      "numel 0\n",
      "Inputs:  {'input_ids': tensor([], device='cuda:0', size=(1, 0)), 'attention_mask': tensor([], device='cuda:0', size=(1, 0)), 'labels': tensor([], device='cuda:0', size=(1, 0))}\n",
      "Inputs - input_ids tensor([], device='cuda:0', size=(1, 0))\n",
      "numel 0\n",
      "Inputs:  {'input_ids': tensor([], device='cuda:0', size=(1, 0)), 'attention_mask': tensor([], device='cuda:0', size=(1, 0)), 'labels': tensor([], device='cuda:0', size=(1, 0))}\n",
      "Inputs - input_ids tensor([], device='cuda:0', size=(1, 0))\n",
      "numel 0\n",
      "Inputs:  {'input_ids': tensor([], device='cuda:0', size=(1, 0)), 'attention_mask': tensor([], device='cuda:0', size=(1, 0)), 'labels': tensor([], device='cuda:0', size=(1, 0))}\n",
      "Inputs - input_ids tensor([], device='cuda:0', size=(1, 0))\n",
      "numel 0\n",
      "Inputs:  {'input_ids': tensor([], device='cuda:0', size=(1, 0)), 'attention_mask': tensor([], device='cuda:0', size=(1, 0)), 'labels': tensor([], device='cuda:0', size=(1, 0))}\n",
      "Inputs - input_ids tensor([], device='cuda:0', size=(1, 0))\n",
      "numel 0\n"
     ]
    }
   ],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d37237a1-7971-491c-a3cf-2b367fdd4999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: lamini_docs_3_steps/final3\n"
     ]
    }
   ],
   "source": [
    "save_dir = f'{output_dir}/final'\n",
    "\n",
    "trainer.save_model(save_dir)\n",
    "print(\"Saved model to:\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24c6dbd1-f76a-4e2c-af33-b99b6167e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ae2c05a-a084-4efe-987b-e9bbda947295",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_slightly_model.to(device) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b6aa527-99e9-4ed1-917a-c9c2911be440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test): Is it possible to fine-tune Lamini on a specific dataset for text generation in legal documents?\n",
      "Finetuned slightly model's answer: \n",
      "Yes, it is possible to fine-tune Lamini on a specific dataset for text generation in legal documents. Lamini is a language model that can be trained on a corpus of text generated by a large corpus of legal documents. Lamini can be fine-tuned on specific datasets or domains to ensure that the data is representative of the desired content. Additionally, L\n"
     ]
    }
   ],
   "source": [
    "test_question = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_question)\n",
    "\n",
    "print(\"Finetuned slightly model's answer: \")\n",
    "print(inference(test_question, finetuned_slightly_model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79e8ac43-3260-4d9d-bba0-fa6c270717ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target answer output (test): Lamini’s LLM Engine can help you fine-tune any model on huggingface or any OpenAI model.\n"
     ]
    }
   ],
   "source": [
    "test_answer = test_dataset[0]['answer']\n",
    "print(\"Target answer output (test):\", test_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (testing_finetune_llm)",
   "language": "python",
   "name": "testing_finetune_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
